# Week 3 Tutorial

For the last time we will look at the Pride and Prejudice (We promise a new dataset from next week ðŸŒŸ )

Below tasks will challenge you to apply (not just repeat) the techniques from badges 7,8 and 9.

## Task: "They talk more about love than family!" with Common Word Embeddings - GloVE ðŸ§¤


You goal: Verify the thesis: More common words in Pride and Prejudice have meaning similar to "Love" than to "Family"?

1. Find some (100? 500? 1000?) most common words in the book (eg. using badge 5 activity "Terms frequency". You might also want to `anti_join(stop_words)`)
2. Create two new columns in which you will store the given words similarity to word `love` and another one for similarity with word `family` (eg. by Using answers answers to badge 7 activity with Glove files)
3. Look at the top values for each term, and decide on a a cut-off point (what similarity value should already not count as similar). Then add occurrences of all good words and all bad words.
4. Validate or deny the thesis.


```{r, include=FALSE}
# IF ASKED ABOTU RESTARTING R SAY: NO
#install.packages("pacman")
pacman::p_load(ggplot2,dplyr, stringr, udpipe, lattice, tidytext, readr, SnowballC, textstem, syuzhet, igraph, tidyr,RColorBrewer,wordcloud)
pacman::p_load(tibble, stringr)
```
and now the data:
```{r}
pride_prejudice_raw_text <- read.delim("./data/pride_prejudice.txt", stringsAsFactor = FALSE)
colnames(pride_prejudice_raw_text)[1] ="text" 
pride_prejudice <- as_tibble(pride_prejudice_raw_text) 
rm(pride_prejudice_raw_text)
```

```{r}
## Term Frequency
# Find the top 100 words (not including stop words)
common_word_counts <-  pride_prejudice %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>%
  top_n(100)
```
[hint: answer to Term Frequency](./hints/hint_9lab_1.Rmd)


Download glove file if you do not have it yet then look at similarity (badge 7)

```{r}
# this is from badge 7:
library(data.table)
options(timeout = 1200)
filename_url <- "https://nlp.stanford.edu/data/wordvecs/glove.2024.wikigiga.50d.zip"
filename_local_zip <- "./data/glove.2024.wikigiga.50d.zip"
filename_local_glove <- "./data/wiki_giga_2024_50_MFT20_vectors_seed_123_alpha_0.75_eta_0.075_combined.txt"

# only get files if we do not have them yet
if (!file.exists(filename_local_glove)){
  if (!file.exists(filename_local_zip)){
    download.file(filename_url, filename_local_zip) 
  }
  unzip(filename_local_zip, exdir="./data/") 
  file.remove(filename_local_zip)
}

IF_LOW_MEMORY_LOAD_JUST_THIS_MANY_WORDS <- 100000 
glove_embeddings <- fread(filename_local_glove, header = FALSE, quote = " ", nrows = IF_LOW_MEMORY_LOAD_JUST_THIS_MANY_WORDS)

setnames(glove_embeddings, c("word", paste0("V", 1:50)))  
glove_matrix <- as.matrix(glove_embeddings[, -1, with = FALSE])
rownames(glove_matrix) <- glove_embeddings$word
```

Note that below operations will take a lot of memory, for people using environment with limited memory (like posit cloud) try to load just the first 100000 lines. (full file has almost 2 million lines).

now do the thing:

```{r}
#  Create two new columns in which you will store the given words similarity to word `love` and another one for similarity with word `family` (eg. by Using answers to badge 7 activity with Glove files)

# Load the get_word_similarity() function from Badge 7

get_word_similarity <- function(word1, word2, glove_words_matrix) {
  # Extract word vectors (inside of the function, yay!)
  vec1 <- glove_words_matrix[word1, ]
  vec2 <- glove_words_matrix[word2, ]
  
  # Compute norms
  norm1 <- sqrt(sum(vec1 * vec1))
  norm2 <- sqrt(sum(vec2 * vec2))
  
  # Compute cosine similarity (same as before)
  similarity <-   sum(vec1 * vec2) / (norm1 * norm2)
  
  return(similarity)
}
```


```{r clean-dataset}
# First, we need to remove any punctuation from our data (e.g. _ or ').
# Otherwise, the get_word_similarity() function will throw an error.

common_words_clean <- common_word_counts |> 
  # Remove all punctuation from the word column
  mutate(word = str_replace_all(word, "[:punct:]", "")) |> 
  # Only keep words which are in the glove words matrix
  filter(word %in% rownames(glove_matrix))
```

Here are two ways to calculate word similarities. The first is done in the tidyverse style and the second in base r. Choose whichever one you prefer (nothing bad happens if you run both).
```{r calculate-word-similarity-tidyverse-solution}
# Calculate word similarity for our common words (tidyverse solution)
common_words_similarities <- common_words_clean |>
  # Tell mutate to calculate for each row
  rowwise() |>
  # Use mutate to add a new column
  mutate(love_similarity = get_word_similarity("love", word, glove_matrix),
         family_similarity = get_word_similarity("family", word, glove_matrix)) |> 
  # Rowwise has grouped the dataset, which will affect future calculations
  ungroup()
```

```{r calculate-word-similarity-base-r-solution}
# Calculate word similarity for our common words (base R solution)
common_words_similarities <- common_words_clean

common_words_similarities$love_similarity <- sapply(common_words_similarities$word, get_word_similarity, word2 = "love", glove_words_matrix = glove_matrix)

common_words_similarities$family_similarity <- sapply(common_words_similarities$word, get_word_similarity, word2 = "family", glove_words_matrix = glove_matrix)
  
```


```{r why-did-I-get-an-error}
# Why did I get an error message in the tutorial? 

# Some of the words contain punctuation (e.g. _ or ') or a made-up word, which the word_similarity function can't cope with

# This will error:
get_word_similarity("love", "_family_", glove_matrix)

# And this:
get_word_similarity("love", "longbourn", glove_matrix)

# And as a result, so will this:
common_word_example1 <- common_word_counts |> 
  mutate(love_similarity = get_word_similarity("love", word, glove_matrix))

# Some of you found that the first 10 rows work fine
# This tells us that the function works, but something is different in later rows
# i.e. why do the first 10 rows work and not the first 100 rows?
common_word_example2 <- common_words_clean |>
  slice(1:10) |> 
  rowwise() |> 
  mutate(love_similarity = get_word_similarity("love", word, glove_matrix)) |> 
  # Rowwise has grouped the dataset, which will affect future calculations
  ungroup()

# Answer: some rows contain punctuation (e.g. _ or ' )

```

Look at the top values for each term, and decide on a a cut-off point (what similarity value should already not count as similar). Then add occurrences of all good words and all bad words.
```{r}
# Top values for "love"
common_words_similarities |> 
  arrange(desc(love_similarity))

# Below 0.6, the words don't seem to be very related to love
love_cutoff <- 0.6

# Top values for "family"
common_words_similarities |> 
  arrange(desc(family_similarity))

# Below 0.6, the words don't seem to be very related to family
family_cutoff <- 0.6

# Add occurrences of all good words and bad words
# i.e. for the top 100 words in Pride & Prejudice, are they related to love or family?

common_words_similarities <- common_words_similarities |> 
  mutate(
    related_to_love = case_when(
      love_similarity >= as.numeric(love_cutoff) ~ "yes",
      .default = "no"
    ),
    related_to_family = case_when(
      family_similarity >= as.numeric(family_cutoff) ~ "yes",
      .default = "no"
    )
  )

# How many words have a meaning similar to "love"?
common_word_similarities |> 
  filter(related_to_love == "yes") |> 
  nrow()

# How many words have a meaning similar to "family"?
common_word_similarities |> 
  filter(related_to_family == "yes") |> 
  nrow()

# How many times do words with a meaning similar to "love" appear in the book?
common_word_similarities |> 
  filter(related_to_love == "yes") |> 
  pull(n) |> 
  sum(na.rm = TRUE)

# How many times do words with a meaning similar to "family" appear in the book?
common_word_similarities |> 
  filter(related_to_family == "yes") |> 
  pull(n) |> 
  sum(na.rm = TRUE)
```
In the first 100 words of Pride and Prejudice, there are **49** unique words related to "love" and **40** unique words related to family (based on our similarity cutoff points for love (0.6) and family (0.6) ).

[hint: answer to counting simmilar words](./hints/hint_9lab_2.Rmd)


## TASK 2: Prouder and More Prejudiced! Improve the entity recognition by cleaning up the dataset better!

In the badge 8 we had a good simple test if the entity recognition recognises every sentence of Elisabeth as a person. We have out 'point of truth'/'perfect answer' by just counting the occurrences of a word `Elizabeth` (634 times). Then we use Entity recognition, and check how many times there a word Elizabeth was recognised as a person (eg. 420 times). If Entity recognition gets very close to simple word count (to 634) then we can be more confident that it works well. In the simple example in badge 8 we saw that without data cleaning, 'person Elisabeth' is recognised 435 times. Add extra cleanup steps below to improve that number, even if you just improve it a little bit.

Here are some ideas: is punctuation removed? what is Elizabeth appears twice in one sentence? (which might be totally legitimate to not put it in twice as an entity). Try to be creative. You are given some starting code below.

```{r, include=FALSE}
pacman::p_load_gh("trinker/entity")
# pacman::p_load(entity, magrittr, tidytext, dplyr, tidyr)
pacman::p_load(dplyr,tidytext)
data(wiki)
person_entity(wiki)
# NOTE! at this point RStudio might ask you a question in 'Console' area at the bottom of your screen.
# Go there and type 'Yes' and click Enter. It will start installing a large-ish file (74.2 MB), which will take 2-3 minutes
```

In the interest of time and memory, if we just look at the top lines, how many uses of Elisabeth we find if counting strings and how many if counting recognised entities.

This is a large piece of data to investigate... and we just don't have time to read the book (just now, I mean. Do read it later, if it's yoru cup of Earl Grey tea!).

Before you start attacking the problem, first write something close to tests. Read starting code below - it creates a column with number of strings "Elizabeth" and another one with count of entitles "Elisabeth". Get crafty, e.g. start by writing some code which will identify (and show) just the lines where those two counts do not match. Then try to identify what could be the reasons, and maybe fix it (if it's easy). 

Treat this exercise as an excuse to get to know the tools and building up some reproducible/trustworthy coding habbits (like testing). Good luck.

```{r}
pride_prejudice_raw_text <- read.delim("./data/pride_prejudice.txt", stringsAsFactor = FALSE)
colnames(pride_prejudice_raw_text)[1] ="text" 
pride_prejudice <- as_tibble(pride_prejudice_raw_text) %>% head(n=500)  
# here we're grabbing just the first x lines, grab less if your memory is sparse

# your code here (warning, it might not be simple)
# 





pride_prejudice$Name <- person_entity(pride_prejudice$text)

some_person = "Elizabeth"

pride_prejudice <- pride_prejudice %>%
  mutate(names_flat = sapply(Name, function(x) paste(x, collapse = " "))) %>%
  mutate(count_strings = str_count(text, fixed(some_person))) %>%
  mutate(count_entitles = str_count(names_flat, fixed(some_person)))

print("counting strings")
pride_prejudice$count_strings %>% sum()
print("counting entities")
pride_prejudice$count_entitles %>% sum()
```

